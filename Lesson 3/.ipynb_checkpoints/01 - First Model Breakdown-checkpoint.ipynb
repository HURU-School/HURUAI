{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6eb4a54",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/HURU-School/HURUAI/blob/main/Lesson%202/07-First%20Model%20Breakdown.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75f0926",
   "metadata": {},
   "source": [
    "# Breaking Down Our First Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6814998",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Setting Up Our Development Enironment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0942bc3",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Mounting Colab to Gdrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0b0e45",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#  Mounts Google Colab on Gdrive.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b90b7e2",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Move to Drive, Create a Working Directory and Move into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362321e7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Selects our Gdrive we just mounted above\n",
    "%cd /content/gdrive/My Drive\n",
    "\n",
    "# Create our working directory\n",
    "%mkdir HuruAI\n",
    "\n",
    "# Move into the working directory\n",
    "%cd /HuruAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d126bf",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603ec29c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# The code below sets us up with some nice formatting for our plots.\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ad147f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1261a60f",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Define a plot funtion that takes an image and returns it's predicted Class.\n",
    "This part is usually not included here. It can be written on a separate page and imported. But I will leave it here so that we do not have to deal with the complexities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692abdcf",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def image_preds(image, probs):\n",
    "    ''' This function is for viewing an image and its predicted class.\n",
    "    '''\n",
    "    probs = probs.data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    ax1.imshow(image.resize_(1, 28, 28).numpy().squeeze())\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(10), probs)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    ax2.set_yticklabels(['T-shirt/top',\n",
    "                            'Trouser',\n",
    "                            'Pullover',\n",
    "                            'Dress',\n",
    "                            'Coat',\n",
    "                            'Sandal',\n",
    "                            'Shirt',\n",
    "                            'Sneaker',\n",
    "                            'Bag',\n",
    "                            'Ankle Boot'], size='small');\n",
    "    ax2.set_title('Returned Class Probabilities')\n",
    "    ax2.set_xlim(0, 1.1)\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0208420a",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Preparing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e68e9bd",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Define Our Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3a43b9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c12b775",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Downloading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b6d013",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('./Data', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('./Data', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf632c4",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Prepare an iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e038d194",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_iterator = iter(trainloader)\n",
    "images, labels = train_iterator.next()\n",
    "print(type(images))\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6227abd8",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can then print out an image from the data loader as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26ff9bc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(images[9].numpy().squeeze(), cmap='Greys_r');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c52b30",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Building Our Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445ffa72",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### The Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb61847d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![Network architecture](../images/Lesson_2/nn.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abcddec",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Building Our Network Purely from Tensors\n",
    "In this section we will explore building the network purely from weight matrices. Next, we will explore using torch's nn module to build the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f11a9c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Define our activation function\n",
    "In the first network, we used a ReLU activation function. For this we will switch things up an explore a new activation function, _the sigmoid activation function_. Mathematically, it is expressed as below:\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "Graphically, the function is represented as below.\n",
    "![Sigmoid Function](../images/Lesson_2/sigmoid.PNG)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c1b946",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Initialize the sigmoid activation function.\n",
    "\n",
    "def activation(x):\n",
    "    return 1/(1+torch.exp(-x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b58f3b",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Flatten the Input Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8545f266",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Flatten the input images\n",
    "inputs = images.view(images.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2de0b2",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Randomly Initialize Our Weights and Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d913fc94",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Initializing Weights and Bias\n",
    "w1 = torch.randn(784, 256)\n",
    "b1 = torch.randn(256)\n",
    "\n",
    "w2 = torch.randn(256, 64)\n",
    "b2 = torch.randn(64)\n",
    "\n",
    "w3 = torch.randn(64, 10)\n",
    "b3 = torch.randn(10)\n",
    "\n",
    "h1 = activation(torch.mm(inputs, w1) + b1)\n",
    "\n",
    "h2 = activation(torch.mm(h1, w2) + b2)\n",
    "\n",
    "out = torch.mm(h2, w3) + b3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc6d49d",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Calculate Probability Distribution\n",
    "The probability distribution is calculated by applying a _softmax function_ across the 10 classes. Mathematically, this function is represented as below:\n",
    "$$\n",
    "\\Large \\sigma(x_i) = \\cfrac{e^{x_i}}{\\sum_k^K{e^{x_k}}}\n",
    "$$\n",
    "It mashes each input x into a range between 0 and 1, then normalizes the values resulting in a proper distribution with the values all adding up to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33120c10",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Define the softmax function\n",
    "\n",
    "def softmax(x):\n",
    "    return torch.exp(x)/torch.sum(torch.exp(x), dim=1).view(-1, 1)\n",
    "\n",
    "probabilities = softmax(out)\n",
    "\n",
    "# Confirm that indeed the shape is (64, 10)\n",
    "print(probabilities.shape)\n",
    "# Confirm that the probabilirs all add up to 1\n",
    "print(probabilities.sum(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c822f7b",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Building the Network with Pytorch's _nn_ Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4dc9d2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Instantiate the Network\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Inputs to hidden layer 1 linear transformation\n",
    "        self.hidden1 = nn.Linear(784, 256)\n",
    "        # Hidden Layer 1 to hidden layer 2 linear transformation\n",
    "        self.hidden2 = nn.Linear(256, 64)\n",
    "        # Output layer, 10 units - one for each item of clothing\n",
    "        self.output = nn.Linear(64, 10)\n",
    "        \n",
    "        # Define sigmoid activation and softmax output \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor through each of our operations\n",
    "        x = self.hidden1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.output(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6045b842",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Initialize the network\n",
    "\n",
    "model = Net()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027ddf9c",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Building the network using Pytorch's *nn.functional* Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdf8f5f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Instantiate the network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Inputs to hidden layer 1 linear transformation\n",
    "        self.hidden1 = nn.Linear(784, 256)\n",
    "        # Hidden Layer 1 to hidden layer 2 linear transformation\n",
    "        self.hidden2 = nn.Linear(256, 64)\n",
    "        # Output layer, 10 units - one for each item of clothing\n",
    "        self.output = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Hidden layer 1 with sigmoid activation\n",
    "        x = F.sigmoid(self.hidden1(x))\n",
    "        # Hidden layer 2 with sigmoid activation\n",
    "        x = F.sigmoid(self.hidden2(x))\n",
    "        # Output layer with softmax activation\n",
    "        x = F.softmax(self.output(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfc5e6b",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Weights and Bias Initialization\n",
    "The weights and biases are initialized, from a random distribution function, for you automatically, unlike when we were building purely from tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69411593",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Check the weights and biases initialized\n",
    "\n",
    "print(model.hidden1.weight)\n",
    "print(model.hidden1.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f54fde2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We could also customize how our weights and biases are initialized as shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6c3b2e",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### Initializing using a constant value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ac71e4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Fill all the bias values with zero\n",
    "model.hidden1.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0407410",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### Initializing by sampling from a distibution function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3367f2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# sample from random normal with standard dev = 0.03\n",
    "model.hidden1.weight.data.normal_(std=0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140038df",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Making our forward pass through the Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5f881d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Grab some data \n",
    "train_iterator = iter(trainloader)\n",
    "images, labels = train_iterator.next()\n",
    "\n",
    "# Resize images into a 1D vector, new shape is (batch size, color channels, image pixels) \n",
    "images.resize_(64, 1, 784)\n",
    "# or images.resize_(images.shape[0], 1, 784) to automatically get batch size\n",
    "\n",
    "# Forward pass through the network\n",
    "image_index = 0\n",
    "probs = model.forward(images[image_index,:])\n",
    "\n",
    "image = images[image_index]\n",
    "image_preds(image.view(1, 28, 28), probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb7a620",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Our Network is not yet trained. As you can see in the plot above, it is just making random guesses. This is because we initialized the weights and biases from a random distribution, hence the random predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d140435",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Building the Network using the nn.Sequential Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f2848e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The nn.Sequential module is unique in that the input tensor is passed sequentially through the transformations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cb3d7d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters for our network\n",
    "input_size = 784\n",
    "hidden_sizes = [256, 64]\n",
    "output_size = 10\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.Softmax(dim=1))\n",
    "print(model)\n",
    "\n",
    "# Forward pass through the network and display output\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(images.shape[0], 1, 784)\n",
    "probs = model.forward(images[0,:])\n",
    "image_preds(images[0].view(1, 28, 28), probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db97bd6",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Training the Network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6718d777",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Define the loss function\n",
    "To train our network, we first need to have some measure of how well the network is performing. We usually calculate a **loss function**, which measures our prediction error. A common loss function used in regression and binary classification problems is the **mean squared loss**, expressed as below:\n",
    "$$\n",
    "\\large \\ell = \\frac{1}{2n}\\sum_i^n{\\left(y_i - \\hat{y}_i\\right)^2}\n",
    "$$\n",
    "\n",
    "where $n$ is the number of training examples, $y_i$ are the true labels, and $\\hat{y}_i$ are the predicted labels. We will be diving deeper into this loss function, but in the meantime, you can find the [documentation for loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions) here.  \n",
    "By continously minimizing this loss, with respect to the network parameters, we will come up with a set of parameters that will give us a minimal loss. The process of finding this minimum loss is called **gradient descent.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb22d7d4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Define Our loss function.\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdafa7a2",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Calculate Gradients\n",
    "Torch provides a module called **Autograd** to automatically calculate gradients of tensors. These gradients are used to update the parameters for our archtecture. Pytorch will automatically initialize all parameters with *require_grad=True*. After we calculate the loss, we can call `loss.backward()` and the gradients are calculated automatically.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f485ad4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Flatten Our images so that we can pass them through a fully connected network\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# Pass the images through our model to get the probabilities\n",
    "logps = model(images)\n",
    "\n",
    "# Calculate the loss\n",
    "loss = criterion(logps, labels)\n",
    "\n",
    "# Let us view the gradients before and after the backward pass.\n",
    "print('Before backward pass: \\n', model[0].weight.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('After backward pass: \\n', model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62761fd2",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Define Our Optimizer Function\n",
    "Torch also provides an _optim_ package to update the weights with the calculated gradients. Optimizers usually require that we pass the parameters we want to optimize, and a learning rate. More on the learning rate soon.  \n",
    "When we do multiple backward passes, the gradients are accumulated. Hence we need to do a `optimizer.zero_grad()` to zero our gradients after every training pass to remove the gradients from the previous training passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5171d02b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Define the optimizer passing in the parameters and a learning rate\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393d8aa3",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Training Process In Full\n",
    "The process of training a neural network is as below:\n",
    "  * We make a forward pass through the network.\n",
    "  * We take the output of the forward pass and use it to calculate our loss\n",
    "  * We perform a backward pass to calculate our gradients\n",
    "  * We take an optimizer step to update the parameters of our architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b445bf",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Instantiate the network Once again to camcel out everything we have just done\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.Softmax(dim=1))\n",
    "\n",
    "# Define Our Loss\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Define our optimization function\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define the number of iterations on the full dataset to make during the training process\n",
    "epochs = 5\n",
    "\n",
    "# Train the Network\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # Flatten our images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "    \n",
    "        # Zero Out the gradients on every training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Calculate the logits(log_probabilies / predictions) generated by the model\n",
    "        output = model(images)\n",
    "        # Calculate our loss\n",
    "        loss = criterion(output, labels)\n",
    "        # Calculate Our gradients to update the model\n",
    "        loss.backward()\n",
    "        # Perform an optimization step to update the parameters(Weights)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track Our loss. It should be decreasing on every iteration\n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962be930",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Testing the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5fd13d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_loss = 0\n",
    "accuracy = 0\n",
    "test_losses = []\n",
    "\n",
    "# Turn off gradients for validation, saves memory and computations\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for images, labels in testloader:\n",
    "        # Flatten our images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        # Pass the image through the model and get the logits\n",
    "        logits = model(images)\n",
    "        # Calculate the test loss\n",
    "        test_loss += criterion(logits, labels)\n",
    "        \n",
    "        # Get the classes for each logit\n",
    "        logit_class = torch.exp(logits)\n",
    "        top_p, top_class = logit_class.topk(1, dim=1)\n",
    "        equals = top_class == labels.view(*top_class.shape)\n",
    "        accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "\n",
    "test_losses.append(test_loss/len(testloader))\n",
    "\n",
    "print(\"Test Loss: {:.3f}.. \".format(test_losses[-1]),\n",
    "      \"Test Accuracy: {:.3f}%\".format((accuracy * 100)/len(testloader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c416a03e",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40adc71",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
